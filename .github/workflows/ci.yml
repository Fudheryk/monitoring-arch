# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# CI optimisÃ©e : unit shardÃ© â†’ (integration âˆ¥ e2e) â†’ aggregate GLOBAL â†’ gate
#
# Corrections / hardening :
# - Coverage : on utilise toujours pytest-cov (coverage CLI seul ne suffit pas)
# - Coverage combine : on combine des fichiers de donnÃ©es coverage (sqlite),
#   pas des fichiers "texte". On Ã©vite de crÃ©er des faux coverage vides.
# - Artifacts : pattern download correct + merge-multiple pour simplifier
# - Logs : diagnostics plus riches en cas de timeout
# - Codecov : condition fiable (secret prÃ©sent)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

name: CI

on:
  pull_request:
  workflow_dispatch:

concurrency:
  group: ci-${{ github.ref }}
  cancel-in-progress: true

permissions:
  contents: read

env:
  # Seuil de couverture configurable
  MIN_COVERAGE: "70"
  # Endpoints CI
  API: http://localhost:8000

jobs:
  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  # 1) UNIT â€” sharding x4 + xdist + coverage par shard
  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  unit:
    runs-on: ubuntu-latest
    timeout-minutes: 15

    services:
      redis:
        image: redis:7-alpine
        ports: ["6379:6379"]
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

    strategy:
      fail-fast: false
      matrix:
        shard: [1, 2, 3, 4]

    env:
      PYTHONPATH: server
      ENV_FILE: /dev/null
      DATABASE_URL: "sqlite+pysqlite:///:memory:"
      REDIS_URL: "redis://localhost:6379/0"
      STUB_SLACK: "1"
      ALERT_REMINDER_MINUTES: "1"

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Cache pip dependencies
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-py311-pip-${{ hashFiles('requirements*.txt') }}-v3
          restore-keys: |
            ${{ runner.os }}-py311-pip-${{ hashFiles('requirements*.txt') }}
            ${{ runner.os }}-py311-pip-

      - name: Install dependencies
        run: |
          set -euo pipefail
          python -m pip install --upgrade pip
          pip install -r requirements-dev.txt
          # DÃ©pendances CI
          pip install pytest-split pytest-xdist pytest-cov coverage

      - name: Verify environment
        run: |
          set -euo pipefail
          echo "PYTHONPATH=$PYTHONPATH"
          python - <<'PY'
          from app.core.config import settings
          print("Settings import OK")
          PY

      - name: Run unit tests (shard ${{ matrix.shard }})
        env:
          CELERY_TASK_ALWAYS_EAGER: "1"
          # Fichier coverage (sans point initial) => artifact facile
          COVERAGE_FILE: "coverage-unit-shard-${{ matrix.shard }}.data"
        run: |
          set -euo pipefail

          echo "â†’ Running shard ${{ matrix.shard }}/4"
          echo "â†’ COVERAGE_FILE=$COVERAGE_FILE"

          # Important : pytest-cov Ã©crit dans COVERAGE_FILE via coverage.py
          if pytest -m "unit or contract" \
                --splits 4 --group ${{ matrix.shard }} \
                -n auto \
                --cov=server/app --cov-config=.coveragerc --cov-branch \
                --cov-report=term-missing:skip-covered --cov-fail-under=0 \
                --maxfail=3 --tb=short; then
            echo "âœ… pytest-split succeeded"
          else
            echo "âš ï¸  pytest-split failed, trying file-based fallback..."

            mapfile -t TEST_FILES < <(find server/tests -name "*test*.py" -type f | sort)
            TOTAL=${#TEST_FILES[@]}

            if [[ $TOTAL -eq 0 ]]; then
              echo "âŒ No test files found"
              exit 1
            fi

            SIZE=$(( (TOTAL + 3) / 4 ))
            START=$(( (${{ matrix.shard }} - 1) * SIZE ))
            SLICE=( "${TEST_FILES[@]:$START:$SIZE}" )

            if [[ ${#SLICE[@]} -eq 0 ]]; then
              echo "âš ï¸  No tests for shard ${{ matrix.shard }} (this is unexpected)."
              # On n'Ã©crit PAS de faux coverage (Ã§a casserait combine). On sort en succÃ¨s
              # seulement si tu acceptes les shards vides (rare). Ici, on Ã©choue :
              exit 1
            fi

            echo "Running ${#SLICE[@]} test files:"
            printf '  %s\n' "${SLICE[@]}"

            pytest -n auto "${SLICE[@]}" -m "unit or contract" \
              --cov=server/app --cov-config=.coveragerc --cov-branch \
              --cov-report=term-missing:skip-covered --cov-fail-under=0 \
              --maxfail=3 --tb=short
          fi

          # VÃ©rifier que le fichier coverage existe et est non vide
          if [[ ! -f "$COVERAGE_FILE" ]]; then
            echo "âŒ Coverage file $COVERAGE_FILE not found"
            ls -la . || true
            exit 1
          fi
          if [[ ! -s "$COVERAGE_FILE" ]]; then
            echo "âŒ Coverage file $COVERAGE_FILE is empty"
            ls -la "$COVERAGE_FILE" || true
            exit 1
          fi

          echo "âœ… Coverage file created: $(ls -lh "$COVERAGE_FILE")"

      - name: Upload shard coverage
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: unit-coverage-shard-${{ matrix.shard }}
          path: coverage-unit-shard-${{ matrix.shard }}.data
          retention-days: 1
          if-no-files-found: error

  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  # 2) INTEGRATION â€” Docker + coverage (host + containers)
  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  integration:
    runs-on: ubuntu-latest
    timeout-minutes: 25

    env:
      STUB_SLACK: "1"
      ALERT_REMINDER_MINUTES: "1"
      API: http://localhost:8000

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Setup Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Cache pip
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-py311-integ-${{ hashFiles('requirements*.txt') }}-v3

      - name: Install dependencies
        run: |
          set -euo pipefail
          python -m pip install -U pip
          pip install -r requirements-dev.txt
          pip install "psycopg[binary]>=3.1,<4.0"
          pip install pytest-cov coverage

      - name: Prepare environment
        run: |
          set -euo pipefail
          cp .env.example .env.docker
          cat >> .env.docker << 'EOF'
          SLACK_WEBHOOK=http://httpbin:80/status/204
          ALERT_REMINDER_MINUTES=1
          STUB_SLACK=1
          EOF
          cp .env.docker docker/.env.docker

      - name: Start Docker stack with coverage
        working-directory: docker
        env:
          API_COVERAGE: "1"
          WORKER_COVERAGE: "1"
          DOCKER_BUILDKIT: "1"
          BUILDKIT_INLINE_CACHE: "1"
        run: |
          set -euo pipefail
          docker compose --env-file ../.env.docker build api worker
          docker compose --env-file ../.env.docker up -d db redis api worker

      - name: Wait for services (DB + migrations + API health)
        working-directory: docker
        shell: bash
        run: |
          set -euo pipefail
          dc() { docker compose --env-file ../.env.docker "$@"; }

          echo "==> Wait DB (pg_isready)"
          for i in {1..60}; do
            if dc exec -T db pg_isready -U postgres >/dev/null 2>&1; then
              echo "âœ… DB ready (attempt $i)"
              break
            fi
            if [[ "$i" -eq 60 ]]; then
              echo "âŒ DB timeout"
              dc ps || true
              dc logs db | tail -200 || true
              exit 1
            fi
            sleep 1
          done

          echo "==> Alembic upgrade head"
          if ! dc run --rm -w /app/server api alembic -c /app/server/alembic.ini upgrade head; then
            echo "âŒ Alembic upgrade failed"
            dc ps || true
            dc logs api | tail -200 || true
            dc logs db | tail -200 || true
            exit 1
          fi

          echo "==> Wait API health (PUBLIC)"
          for i in {1..60}; do
            if curl -fsS -m 2 "${API}/api/v1/health" >/dev/null 2>&1; then
              echo "âœ… API healthy (attempt $i)"
              break
            fi
            if [[ "$i" -eq 60 ]]; then
              echo "âŒ API timeout"
              dc ps || true
              dc logs api | tail -200 || true
              dc logs worker | tail -200 || true
              dc logs db | tail -200 || true
              exit 1
            fi
            sleep 1
          done

      - name: Run integration tests (host) with coverage
        env:
          PYTHONPATH: server
          DATABASE_URL: "postgresql+psycopg://postgres:postgres@localhost:5432/monitoring?connect_timeout=10"
          COVERAGE_FILE: "coverage-integration-host.data"
        run: |
          set -euo pipefail
          pytest -m "integration" -v --tb=short \
            --cov=server/app --cov-config=.coveragerc --cov-branch \
            --cov-report=term-missing:skip-covered --cov-fail-under=0

      - name: Stop coverage services and collect container coverage
        working-directory: docker
        shell: bash
        run: |
          set -euo pipefail
          dc() { docker compose --env-file ../.env.docker "$@"; }

          echo "Stopping services to flush coverage..."
          dc stop api worker || true
          sleep 2

          # Les fichiers coverage des containers doivent Ãªtre produits par lâ€™entrypoint
          # (ex: COVERAGE_FILE=/app/server/coverage-*.data) quand API_COVERAGE/WORKER_COVERAGE=1
          echo "Copying coverage files from containers..."
          dc cp api:/app/server/coverage-api-container.data ../coverage-integration-api.data || echo "âš ï¸  No API coverage file"
          dc cp worker:/app/server/coverage-worker-container.data ../coverage-integration-worker.data || echo "âš ï¸  No worker coverage file"

      - name: Combine integration coverage
        run: |
          set -euo pipefail

          files=()
          for f in coverage-integration-host.data coverage-integration-api.data coverage-integration-worker.data; do
            if [[ -f "$f" && -s "$f" ]]; then
              files+=("$f")
            fi
          done

          if [[ ${#files[@]} -eq 0 ]]; then
            echo "âŒ No integration coverage files to combine"
            ls -la . || true
            exit 1
          fi

          echo "Combining integration files:"
          printf ' - %s\n' "${files[@]}"

          COVERAGE_FILE=coverage-integration-combined.data coverage combine "${files[@]}"
          COVERAGE_FILE=coverage-integration-combined.data coverage xml -o coverage-integration.xml
          COVERAGE_FILE=coverage-integration-combined.data coverage html -d htmlcov-integration
          COVERAGE_FILE=coverage-integration-combined.data coverage report --skip-covered

      - name: Upload integration coverage data
        uses: actions/upload-artifact@v4
        with:
          name: integration-coverage-data
          path: coverage-integration-combined.data
          retention-days: 1

      - name: Cleanup
        if: always()
        working-directory: docker
        run: docker compose --env-file ../.env.docker down -v --remove-orphans

  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  # 3) E2E â€” Docker + coverage (host + api container)
  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  e2e:
    runs-on: ubuntu-latest
    timeout-minutes: 15

    env:
      STUB_SLACK: "1"
      ALERT_REMINDER_MINUTES: "1"
      API: http://localhost:8000

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Setup Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: |
          set -euo pipefail
          python -m pip install -U pip
          pip install -r requirements-dev.txt
          pip install "psycopg[binary]>=3.1,<4.0"
          pip install pytest-cov coverage

      - name: Prepare environment
        run: |
          set -euo pipefail
          cp .env.example .env.docker
          cat >> .env.docker << 'EOF'
          SLACK_WEBHOOK=http://httpbin:80/status/204
          ALERT_REMINDER_MINUTES=1
          STUB_SLACK=1
          EOF
          cp .env.docker docker/.env.docker

      - name: Start services (db/redis/api only)
        working-directory: docker
        env:
          API_COVERAGE: "1"
          DOCKER_BUILDKIT: "1"
          BUILDKIT_INLINE_CACHE: "1"
        run: |
          set -euo pipefail
          docker compose --env-file ../.env.docker up -d --build db redis api

      - name: Wait for services (DB + migrations + API health)
        working-directory: docker
        shell: bash
        run: |
          set -euo pipefail
          dc() { docker compose --env-file ../.env.docker "$@"; }

          echo "==> Wait DB (pg_isready)"
          for i in {1..60}; do
            if dc exec -T db pg_isready -U postgres >/dev/null 2>&1; then
              echo "âœ… DB ready (attempt $i)"
              break
            fi
            if [[ "$i" -eq 60 ]]; then
              echo "âŒ DB timeout"
              dc ps || true
              dc logs db | tail -200 || true
              exit 1
            fi
            sleep 1
          done

          echo "==> Alembic upgrade head"
          if ! dc run --rm -w /app/server api alembic -c /app/server/alembic.ini upgrade head; then
            echo "âŒ Alembic upgrade failed"
            dc ps || true
            dc logs api | tail -200 || true
            dc logs db | tail -200 || true
            exit 1
          fi

          echo "==> Wait API health (PUBLIC)"
          for i in {1..60}; do
            if curl -fsS -m 2 "${API}/api/v1/health" >/dev/null 2>&1; then
              echo "âœ… API healthy (attempt $i)"
              break
            fi
            if [[ "$i" -eq 60 ]]; then
              echo "âŒ API timeout"
              dc ps || true
              dc logs api | tail -200 || true
              exit 1
            fi
            sleep 1
          done

      - name: Run smoke tests
        if: hashFiles('scripts/smoke_http_targets.sh') != ''
        env:
          # Ton script smoke lit KEY cÃ´tÃ© env : en CI, assure-toi que KEY est bien fourni.
          # Si tu veux "0 traces" cÃ´tÃ© CI, fais en sorte que la clÃ© seedÃ©e soit injectÃ©e
          # via secrets / env, pas en dur dans le repo.
          KEY: ${{ env.KEY }}
        run: |
          set -euo pipefail
          if [[ -x scripts/smoke_http_targets.sh ]]; then
            bash scripts/smoke_http_targets.sh
          else
            echo "âš ï¸  Smoke script not executable"
            exit 1
          fi

      - name: Run E2E tests (host) with coverage
        env:
          PYTHONPATH: server
          DATABASE_URL: "postgresql+psycopg://postgres:postgres@localhost:5432/monitoring?connect_timeout=10"
          COVERAGE_FILE: "coverage-e2e-host.data"
          E2E_STACK_UP: "1"
        run: |
          set -euo pipefail
          pytest -m "e2e" -v --tb=short \
            --cov=server/app --cov-config=.coveragerc --cov-branch \
            --cov-report=term-missing:skip-covered --cov-fail-under=0

      - name: Stop API and collect container coverage
        working-directory: docker
        shell: bash
        run: |
          set -euo pipefail
          dc() { docker compose --env-file ../.env.docker "$@"; }

          dc stop api || true
          sleep 2
          dc cp api:/app/server/coverage-e2e-api.data ../coverage-e2e-api-container.data || echo "âš ï¸  No E2E API coverage file"

      - name: Combine E2E coverage
        run: |
          set -euo pipefail

          files=()
          for f in coverage-e2e-host.data coverage-e2e-api-container.data; do
            if [[ -f "$f" && -s "$f" ]]; then
              files+=("$f")
            fi
          done

          if [[ ${#files[@]} -eq 0 ]]; then
            echo "âŒ No E2E coverage files"
            ls -la . || true
            exit 1
          fi

          COVERAGE_FILE=coverage-e2e-combined.data coverage combine "${files[@]}"
          COVERAGE_FILE=coverage-e2e-combined.data coverage xml -o coverage-e2e.xml
          COVERAGE_FILE=coverage-e2e-combined.data coverage html -d htmlcov-e2e
          COVERAGE_FILE=coverage-e2e-combined.data coverage report --skip-covered

      - name: Upload E2E coverage data
        uses: actions/upload-artifact@v4
        with:
          name: e2e-coverage-data
          path: coverage-e2e-combined.data
          retention-days: 1

      - name: Cleanup
        if: always()
        working-directory: docker
        run: docker compose --env-file ../.env.docker down -v --remove-orphans

  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  # 4) AGRÃ‰GATION GLOBALE â€” Unit + Integration + E2E + gate coverage
  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  aggregate-coverage:
    name: aggregate-all-coverage
    runs-on: ubuntu-latest
    needs: [unit, integration, e2e]
    timeout-minutes: 10

    steps:
      - uses: actions/checkout@v4

      - name: Setup Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install coverage
        run: |
          set -euo pipefail
          pip install coverage

      - name: Create staging directory
        run: mkdir -p coverage-staging/unit coverage-staging/integration coverage-staging/e2e

      - name: Download unit coverage artifacts
        uses: actions/download-artifact@v4
        with:
          pattern: unit-coverage-shard-*
          path: coverage-staging/unit
          merge-multiple: true

      - name: Download integration coverage
        uses: actions/download-artifact@v4
        with:
          name: integration-coverage-data
          path: coverage-staging/integration

      - name: Download E2E coverage
        uses: actions/download-artifact@v4
        with:
          name: e2e-coverage-data
          path: coverage-staging/e2e

      - name: Verify all downloads
        run: |
          set -euo pipefail
          echo "=== Downloaded coverage files ==="
          find coverage-staging -type f -maxdepth 3 -ls || true

      - name: Combine ALL coverage (unit + integration + e2e) + enforce threshold
        env:
          MIN_COVERAGE: ${{ env.MIN_COVERAGE }}
        run: |
          set -euo pipefail

          files=()
          while IFS= read -r -d '' f; do
            [[ -s "$f" ]] && files+=("$f")
          done < <(find coverage-staging -type f -name "*.data" -print0)

          if [[ ${#files[@]} -eq 0 ]]; then
            echo "âŒ No coverage files found for global aggregation"
            exit 1
          fi

          echo "=== Combining ${#files[@]} coverage files ==="
          printf ' - %s\n' "${files[@]}"

          # Important : en mettant COVERAGE_FILE, on contrÃ´le la sortie (.coverage ou autre)
          COVERAGE_FILE=coverage-global.data coverage combine "${files[@]}"

          echo "=== GLOBAL COVERAGE REPORT ==="
          COVERAGE_FILE=coverage-global.data coverage report --skip-covered --show-missing --fail-under="${MIN_COVERAGE}"

          COVERAGE_FILE=coverage-global.data coverage xml -o coverage-global.xml
          COVERAGE_FILE=coverage-global.data coverage html -d htmlcov-global

      - name: Upload global coverage XML
        uses: actions/upload-artifact@v4
        with:
          name: coverage-global-xml
          path: coverage-global.xml
          retention-days: 7

      - name: Upload global coverage data
        uses: actions/upload-artifact@v4
        with:
          name: coverage-global-data
          path: coverage-global.data
          retention-days: 7

      - name: Upload global HTML report
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: htmlcov-global
          path: htmlcov-global/
          if-no-files-found: ignore
          retention-days: 7

      - name: Upload to Codecov (if configured)
        if: ${{ secrets.CODECOV_TOKEN != '' }}
        env:
          CODECOV_TOKEN: ${{ secrets.CODECOV_TOKEN }}
        run: |
          set -euo pipefail
          curl -s https://codecov.io/bash | bash -s -- -f coverage-global.xml

  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  # 5) GATE â€” job final (statut unique)
  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  gate:
    name: ci-gate
    runs-on: ubuntu-latest
    needs: [unit, integration, e2e, aggregate-coverage]
    timeout-minutes: 5

    steps:
      - name: All checks passed
        run: |
          set -euo pipefail
          echo "ğŸ‰ ALL CI JOBS COMPLETED SUCCESSFULLY!"
          echo "âœ… Unit tests (4 shards) - PASSED"
          echo "âœ… Integration tests - PASSED"
          echo "âœ… E2E tests - PASSED"
          echo "âœ… Global coverage aggregation (>= ${MIN_COVERAGE}%) - PASSED"

      - name: Download final coverage report
        uses: actions/download-artifact@v4
        with:
          name: coverage-global-xml
          path: .

      - name: Display coverage summary
        run: |
          set -euo pipefail
          if [[ -f coverage-global.xml ]]; then
            python3 - <<'PY'
            import xml.etree.ElementTree as ET
            root = ET.parse("coverage-global.xml").getroot()
            line_rate = float(root.get("line-rate", 0)) * 100
            branch_rate = float(root.get("branch-rate", 0)) * 100
            print(f"Lines covered: {line_rate:.1f}%")
            print(f"Branches covered: {branch_rate:.1f}%")
            PY
          else
            echo "âš ï¸  coverage-global.xml not found"
            exit 1
          fi

      - name: Quality gate check
        run: |
          set -euo pipefail
          echo "ğŸšª All quality gates passed - ready for merge! ğŸš€"
