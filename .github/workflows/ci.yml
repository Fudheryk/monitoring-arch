# ─────────────────────────────────────────────────────────────────────────────
# CI optimisée : unit shardé → (integration ∥ e2e) → aggregate → gate + reports
# 
# AMÉLIORATIONS:
# - Timeout explicites pour éviter les jobs qui traînent
# - Gestion d'erreur renforcée sur la coverage
# - Upload conditionnel des rapports HTML
# - Cache Docker plus agressif
# - Retry sur les étapes critiques (DB, API)
# ──────────────────────────────────────────────────────────────────────────────

name: CI

on:
  push:
    branches: ['**']
  pull_request:

concurrency:
  group: ci-${{ github.ref }}
  cancel-in-progress: true

permissions:
  contents: read

jobs:

  # ────────────────────────────────────────────────────────────────────────────
  # 1) UNIT — sharding x4 + xdist + coverage par shard
  # ────────────────────────────────────────────────────────────────────────────
  unit:
    runs-on: ubuntu-latest
    timeout-minutes: 15  # Timeout explicite

    services:
      redis:
        image: redis:7-alpine
        ports: [ "6379:6379" ]
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

    strategy:
      fail-fast: false
      matrix:
        shard: [1, 2, 3, 4]

    env:
      PYTHONPATH: server
      ENV_FILE: /dev/null
      DATABASE_URL: 'sqlite+pysqlite:///:memory:'
      REDIS_URL: redis://localhost:6379/0
      STUB_SLACK: "1"
      ALERT_REMINDER_MINUTES: "1"
      KEY: dev-apikey-123
      API: http://localhost:8000

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      # Cache pip plus agressif
      - name: Cache pip dependencies
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-py311-pip-${{ hashFiles('requirements*.txt') }}-v2
          restore-keys: |
            ${{ runner.os }}-py311-pip-${{ hashFiles('requirements*.txt') }}
            ${{ runner.os }}-py311-pip-

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements-dev.txt
          pip install pytest-split

      - name: Verify environment
        run: |
          echo "PYTHONPATH=$PYTHONPATH"
          python -c "
          import sys; 
          print('Python path OK'); 
          from app.core.config import settings; 
          print('Settings import OK')
          "

      - name: Run unit tests (shard ${{ matrix.shard }})
        env:
          CELERY_TASK_ALWAYS_EAGER: "1"
          COVERAGE_FILE: ".coverage.unit.${{ matrix.shard }}"
        run: |
          set -euo pipefail
          
          echo "→ Running shard ${{ matrix.shard }}/4 with coverage file: $COVERAGE_FILE"
          
          # Tentative pytest-split d'abord
          if pytest -m "unit" --splits 4 --group ${{ matrix.shard }} -n auto \
                --cov=server/app --cov-config=.coveragerc --cov-branch \
                --cov-report=term-missing:skip-covered --cov-fail-under=0 \
                --maxfail=3 --tb=short; then
            echo "✅ pytest-split succeeded"
          else
            echo "⚠️  pytest-split failed, trying file-based fallback..."
            
            mapfile -t TEST_FILES < <(find server/tests -name "*test*.py" -type f | sort)
            TOTAL=${#TEST_FILES[@]}
            
            if [[ $TOTAL -eq 0 ]]; then 
              echo "❌ No test files found"
              exit 1
            fi
            
            SIZE=$(( (TOTAL + 3) / 4 ))
            START=$(( (${{ matrix.shard }} - 1) * SIZE ))
            SLICE=( "${TEST_FILES[@]:$START:$SIZE}" )
            
            if [[ ${#SLICE[@]} -eq 0 ]]; then
              echo "⚠️  No tests for shard ${{ matrix.shard }}, creating empty coverage"
              echo "# Empty shard" > "$COVERAGE_FILE"
            else
              echo "Running ${#SLICE[@]} test files:"
              printf '  %s\n' "${SLICE[@]}"
              
              pytest -n auto "${SLICE[@]}" -m "unit" \
                --cov=server/app --cov-config=.coveragerc --cov-branch \
                --cov-report=term-missing:skip-covered --cov-fail-under=0 \
                --maxfail=3 --tb=short
            fi
          fi

          # Vérification du fichier de coverage
          if [[ -f .coverage && ! -f "$COVERAGE_FILE" ]]; then 
            mv .coverage "$COVERAGE_FILE"
          fi
          
          if [[ ! -f "$COVERAGE_FILE" ]]; then
            echo "❌ Coverage file $COVERAGE_FILE not found"
            ls -la .coverage* || true
            exit 1
          fi
          
          echo "✅ Coverage file created: $(ls -lh "$COVERAGE_FILE")"

      - name: Upload shard coverage
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: coverage-shard-${{ matrix.shard }}
          path: .coverage.unit.${{ matrix.shard }}
          retention-days: 1
          if-no-files-found: error

  # ────────────────────────────────────────────────────────────────────────────
  # 2) INTEGRATION — Docker + coverage + retry sur échecs critiques
  # ────────────────────────────────────────────────────────────────────────────
  integration:
    runs-on: ubuntu-latest
    timeout-minutes: 20
    
    env:
      STUB_SLACK: "1"
      ALERT_REMINDER_MINUTES: "1"
      KEY: dev-apikey-123
      API: http://localhost:8000

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Docker Buildx
        uses: docker/setup-buildx-action@v3

      # Cache Docker plus agressif
      - name: Cache Docker layers
        uses: actions/cache@v4
        with:
          path: /tmp/.buildx-cache
          key: ${{ runner.os }}-docker-${{ hashFiles('server/Dockerfile', 'requirements*.txt', 'docker/*') }}-v2
          restore-keys: |
            ${{ runner.os }}-docker-${{ hashFiles('server/Dockerfile', 'requirements*.txt') }}
            ${{ runner.os }}-docker-

      - name: Setup Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Cache pip
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-py311-integ-${{ hashFiles('requirements*.txt') }}-v2

      - name: Install dependencies
        run: |
          python -m pip install -U pip
          pip install -r requirements-dev.txt
          pip install "psycopg[binary]>=3.1,<4.0"

      - name: Prepare environment
        run: |
          cp .env.example .env.docker
          cat >> .env.docker << 'EOF'
          SLACK_WEBHOOK=http://httpbin:80/status/204
          ALERT_REMINDER_MINUTES=1
          STUB_SLACK=1
          EOF
          cp .env.docker docker/.env.docker

      - name: Start Docker stack with coverage
        working-directory: docker
        env:
          API_COVERAGE: "1"
          WORKER_COVERAGE: "1"
          COVERAGE_FILE: "/app/server/.coverage.api"
          DOCKER_BUILDKIT: 1
          BUILDKIT_INLINE_CACHE: 1
        run: |
          # Build avec cache
          docker compose --env-file ../.env.docker build \
            --build-arg BUILDKIT_INLINE_CACHE=1 api
          
          # Start services
          docker compose --env-file ../.env.docker up -d db redis api worker

      # Retry sur DB ready
      - name: Wait for database
        working-directory: docker
        run: |
          echo "Waiting for PostgreSQL to be ready..."
          for attempt in {1..30}; do
            if docker compose --env-file ../.env.docker exec -T db pg_isready -U postgres -d postgres; then
              echo "✅ Database ready after ${attempt}s"
              break
            fi
            if [[ $attempt -eq 30 ]]; then
              echo "❌ Database failed to start"
              docker compose logs db
              exit 1
            fi
            echo "Attempt $attempt/30..."
            sleep 1
          done

      - name: Run migrations
        working-directory: docker
        run: |
          docker compose --env-file ../.env.docker run --rm -w /app/server api \
            alembic -c /app/server/alembic.ini upgrade head

      # Retry sur API health
      - name: Wait for API health
        run: |
          echo "Waiting for API to be healthy..."
          for attempt in {1..30}; do
            if curl -fsS -m 5 -H "X-API-Key: $KEY" "$API/api/v1/health" >/dev/null 2>&1; then
              echo "✅ API healthy after ${attempt}s"
              break
            fi
            if [[ $attempt -eq 30 ]]; then
              echo "❌ API failed to be healthy"
              docker compose -f docker/docker-compose.yml logs api | tail -50
              exit 1
            fi
            echo "Attempt $attempt/30..."
            sleep 1
          done

      - name: Run integration tests
        env:
          PYTHONPATH: server
          DATABASE_URL: 'postgresql+psycopg://postgres:postgres@localhost:5432/monitoring?connect_timeout=10'
          COVERAGE_FILE: ".coverage.integ.host"
        run: |
          pytest -m "integration" -v --tb=short \
            --cov=server/app --cov-config=.coveragerc --cov-branch \
            --cov-report=term-missing:skip-covered --cov-fail-under=0

      - name: Stop coverage services
        working-directory: docker
        run: |
          echo "Stopping services to flush coverage..."
          docker compose --env-file ../.env.docker stop api worker || true
          sleep 2  # Laisser le temps au flush

      - name: Combine coverage files
        run: |
          set -euo pipefail
          
          echo "=== Available coverage files ==="
          find . -maxdepth 2 -name ".coverage*" -type f -exec ls -lh {} \;
          
          # Collecte des fichiers non vides
          files=()
          
          if [[ -f ".coverage.integ.host" ]] && [[ -s ".coverage.integ.host" ]]; then
            files+=(".coverage.integ.host")
          fi
          
          # Fichiers des conteneurs
          while IFS= read -r -d '' file; do
            if [[ -s "$file" ]]; then
              files+=("$file")
            fi
          done < <(find server -maxdepth 1 -name ".coverage*" ! -name ".coveragerc" -type f -print0 2>/dev/null || true)
          
          if [[ ${#files[@]} -eq 0 ]]; then
            echo "❌ No coverage files to combine"
            exit 1
          fi
          
          echo "=== Combining ${#files[@]} files ==="
          printf '%s\n' "${files[@]}"
          
          COVERAGE_FILE=.coverage coverage combine "${files[@]}"
          COVERAGE_FILE=.coverage coverage xml -o coverage-integration.xml
          COVERAGE_FILE=.coverage coverage html -d htmlcov-integration
          COVERAGE_FILE=.coverage coverage report --skip-covered
          
          echo "✅ Integration coverage: $(ls -lh coverage-integration.xml)"

      - name: Upload coverage XML
        uses: actions/upload-artifact@v4
        with:
          name: coverage-integration-xml
          path: coverage-integration.xml

      - name: Upload HTML report
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: htmlcov-integration
          path: htmlcov-integration/
          if-no-files-found: ignore

      - name: Cleanup
        if: always()
        working-directory: docker
        run: docker compose --env-file ../.env.docker down -v --remove-orphans

  # ────────────────────────────────────────────────────────────────────────────
  # 3) E2E — similaire à integration mais sans worker + smoke tests
  # ────────────────────────────────────────────────────────────────────────────
  e2e:
    runs-on: ubuntu-latest
    timeout-minutes: 15
    
    env:
      STUB_SLACK: "1"
      ALERT_REMINDER_MINUTES: "1"
      KEY: dev-apikey-123
      API: http://localhost:8000

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Cache Docker layers
        uses: actions/cache@v4
        with:
          path: /tmp/.buildx-cache
          key: ${{ runner.os }}-docker-${{ hashFiles('server/Dockerfile', 'requirements*.txt', 'docker/*') }}-v2
          restore-keys: |
            ${{ runner.os }}-docker-${{ hashFiles('server/Dockerfile', 'requirements*.txt') }}
            ${{ runner.os }}-docker-

      - name: Setup Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install -U pip
          pip install -r requirements-dev.txt
          pip install "psycopg[binary]>=3.1,<4.0"

      - name: Prepare environment
        run: |
          cp .env.example .env.docker
          cat >> .env.docker << 'EOF'
          SLACK_WEBHOOK=http://httpbin:80/status/204
          ALERT_REMINDER_MINUTES=1
          STUB_SLACK=1
          EOF
          cp .env.docker docker/.env.docker

      - name: Start services (db/redis/api only)
        working-directory: docker
        env:
          API_COVERAGE: "1"
          COVERAGE_FILE: "/app/server/.coverage.api"
          DOCKER_BUILDKIT: 1
          BUILDKIT_INLINE_CACHE: 1
        run: |
          docker compose --env-file ../.env.docker up -d --build db redis api

      - name: Wait for services
        working-directory: docker
        run: |
          # DB
          for i in {1..30}; do
            if docker compose --env-file ../.env.docker exec -T db pg_isready -U postgres; then
              break
            fi
            [[ $i -eq 30 ]] && { echo "❌ DB timeout"; docker compose logs db; exit 1; }
            sleep 1
          done
          
          # Migrations
          docker compose --env-file ../.env.docker run --rm -w /app/server api \
            alembic -c /app/server/alembic.ini upgrade head
          
          # API
          for i in {1..30}; do
            if curl -fsS -m 5 -H "X-API-Key: ${{ env.KEY }}" "${{ env.API }}/api/v1/health" >/dev/null; then
              break
            fi
            [[ $i -eq 30 ]] && { echo "❌ API timeout"; docker compose logs api | tail -30; exit 1; }
            sleep 1
          done

      - name: Run smoke tests
        if: hashFiles('scripts/smoke_http_targets.sh') != ''
        run: |
          if [[ -x scripts/smoke_http_targets.sh ]]; then
            bash scripts/smoke_http_targets.sh
          else
            echo "⚠️  Smoke script not found or not executable"
          fi

      - name: Run E2E tests
        env:
          PYTHONPATH: server
          DATABASE_URL: 'postgresql+psycopg://postgres:postgres@localhost:5432/monitoring?connect_timeout=10'
          COVERAGE_FILE: ".coverage.e2e.host"
        run: |
          pytest -m "e2e" -v --tb=short \
            --cov=server/app --cov-config=.coveragerc --cov-branch \
            --cov-report=term-missing:skip-covered --cov-fail-under=0

      - name: Stop API for coverage flush
        working-directory: docker
        run: |
          docker compose --env-file ../.env.docker stop api || true
          sleep 2

      - name: Combine coverage
        run: |
          set -euo pipefail
          
          files=()
          [[ -f ".coverage.e2e.host" && -s ".coverage.e2e.host" ]] && files+=(".coverage.e2e.host")
          
          while IFS= read -r -d '' file; do
            [[ -s "$file" ]] && files+=("$file")
          done < <(find server -maxdepth 1 -name ".coverage*" ! -name ".coveragerc" -type f -print0 2>/dev/null || true)
          
          [[ ${#files[@]} -eq 0 ]] && { echo "❌ No coverage files"; exit 1; }
          
          echo "Combining: ${files[*]}"
          COVERAGE_FILE=.coverage coverage combine "${files[@]}"
          COVERAGE_FILE=.coverage coverage xml -o coverage-e2e.xml
          COVERAGE_FILE=.coverage coverage html -d htmlcov-e2e
          echo "✅ E2E coverage: $(ls -lh coverage-e2e.xml)"

      - name: Upload coverage XML
        uses: actions/upload-artifact@v4
        with:
          name: coverage-e2e-xml
          path: coverage-e2e.xml

      - name: Upload HTML report
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: htmlcov-e2e
          path: htmlcov-e2e/
          if-no-files-found: ignore

      - name: Cleanup
        if: always()
        working-directory: docker
        run: docker compose --env-file ../.env.docker down -v --remove-orphans

  # ────────────────────────────────────────────────────────────────────────────
  # 4) AGGREGATE UNIT — combine les shards de manière robuste
  # ────────────────────────────────────────────────────────────────────────────
  aggregate-unit-coverage:
    runs-on: ubuntu-latest
    needs: unit
    timeout-minutes: 5

    steps:
      - uses: actions/checkout@v4

      - name: Setup Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install coverage
        run: pip install coverage

      - name: Create staging directory
        run: mkdir -p coverage-staging

      # Téléchargements explicites pour éviter les conflits
      - name: Download all shard artifacts
        uses: actions/download-artifact@v4
        with:
          pattern: coverage-shard-*
          path: coverage-staging/
          merge-multiple: false

      - name: Verify downloads
        run: |
          echo "=== Downloaded artifacts ==="
          find coverage-staging -type f -ls
          
          echo -e "\n=== Unit coverage files ==="
          find coverage-staging -name ".coverage.unit.*" -type f -exec ls -lh {} \;

      - name: Combine unit coverage
        run: |
          set -euo pipefail
          
          # Collecte de tous les fichiers unit non vides
          files=()
          while IFS= read -r -d '' file; do
            if [[ -s "$file" ]]; then
              files+=("$file")
            fi
          done < <(find coverage-staging -name ".coverage.unit.*" -type f -print0)
          
          if [[ ${#files[@]} -eq 0 ]]; then
            echo "❌ No unit coverage files found"
            echo "Check that unit job succeeded and uploaded artifacts"
            exit 1
          fi
          
          echo "=== Combining ${#files[@]} unit coverage files ==="
          printf '%s\n' "${files[@]}"
          
          # Combine
          coverage combine "${files[@]}"
          
          # Reports
          coverage report --skip-covered --show-missing
          coverage xml -o coverage-unit.xml  
          coverage html -d htmlcov-unit
          
          echo "✅ Unit coverage combined: $(ls -lh coverage-unit.xml)"

      - name: Upload combined XML
        uses: actions/upload-artifact@v4
        with:
          name: coverage-unit-xml
          path: coverage-unit.xml

      - name: Upload HTML report
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: htmlcov-unit
          path: htmlcov-unit/
          if-no-files-found: ignore

  # ────────────────────────────────────────────────────────────────────────────
  # 5) GATE — Point de passage unique pour les protections de branche
  # ────────────────────────────────────────────────────────────────────────────
  gate:
    name: ci-gate
    runs-on: ubuntu-latest
    needs: [unit, aggregate-unit-coverage, integration, e2e]
    timeout-minutes: 2
    
    steps:
      - name: All checks passed
        run: |
          echo "🎉 All CI jobs completed successfully!"
          echo "✅ Unit tests (4 shards) - PASSED"
          echo "✅ Integration tests - PASSED" 
          echo "✅ E2E tests - PASSED"
          echo "✅ Coverage aggregation - PASSED"

      # Optionnel: télécharger les coverage pour un rapport final
      - name: Download coverage reports
        if: github.ref == 'refs/heads/main'
        uses: actions/download-artifact@v4
        with:
          pattern: coverage-*-xml
          path: final-coverage/
          merge-multiple: true

      - name: Final coverage summary
        if: github.ref == 'refs/heads/main'
        run: |
          echo "=== Final Coverage Files ==="
          find final-coverage -name "*.xml" -exec basename {} \; | sort
          
          # Ici vous pourriez uploader vers CodeCov, SonarQube, etc.
          # curl -s https://codecov.io/bash | bash -s -- -t $CODECOV_TOKEN