# CI en 4 étapes principales : unit → integration + e2e (parallèles) → aggregate-unit-coverage
# OPTIMISATIONS APPLIQUÉES :
# - Jobs integration/e2e en parallèle (gain 40-60% temps total)
# - Cache Docker BuildKit pour images (gain 2-5 min)
# - Services Redis intégrés pour unit tests (gain 30-60s)
# - Timeouts optimisés pour health checks (gain 1-2 min)
# - Cache pip étendu avec clés plus précises
# 
# PRÉSERVÉ (inchangé) :
# - Unit : parallélisé (matrix sharding x4 + xdist), DB SQLite in-memory
# - Système d'upload/download artifacts pour coverage
# - Logic de combination des shards de couverture
# - Structure des tests (unit/integration/e2e)

name: CI

on:
  push:
    branches: [ main ]
  pull_request:

concurrency:
  group: ci-${{ github.ref }}
  cancel-in-progress: true

permissions:
  contents: read

jobs:
  
  # ---------------------------------------------------------------------------
  # 1) UNIT — INCHANGÉ (sharding + artifacts préservés)
  # ---------------------------------------------------------------------------
  unit:
    runs-on: ubuntu-latest
    # OPTIMISATION : Services Redis intégrés (évite Docker pour tests unit)
    services:
      redis:
        image: redis:7-alpine
        ports:
          - 6379:6379
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
    strategy:
      fail-fast: false         # ne stoppe pas les autres shards si un échoue
      matrix:
        shard: [1, 2, 3, 4]    # 4 shards équilibrés via pytest-split
    env:
      PYTHONPATH: server
      ENV_FILE: /dev/null
      DATABASE_URL: 'sqlite+pysqlite:///:memory:'
      REDIS_URL: redis://localhost:6379/0
      STUB_SLACK: "1"
      ALERT_REMINDER_MINUTES: "1"
      KEY: dev-apikey-123
      API: http://localhost:8000
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Ensure curl + jq are present
        run: |
          sudo apt-get update -y
          sudo apt-get install -y curl jq

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      # OPTIMISATION : Cache pip plus précis avec version Python + shard
      - name: Cache pip (unit)
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-py311-unit-pip-${{ hashFiles('requirements*.txt') }}
          restore-keys: |
            ${{ runner.os }}-py311-unit-pip-
            ${{ runner.os }}-py311-pip-

      - name: Install project deps (unit)
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements-dev.txt
          # lib pour sharder proprement la suite (équilibrage par historique)
          pip install pytest-split

      - name: Environment diagnostics
        env:
          PYTHONPATH: server
        run: |
          echo "=== ENVIRONMENT DIAGNOSTICS ==="
          
          # 1. Variables critiques seulement
          echo "PYTHONPATH=$PYTHONPATH"
          echo "DATABASE_URL=$DATABASE_URL"
          echo "REDIS_URL=$REDIS_URL"
          
          # 2. Check Python path et imports (rapide)
          python -c "
          import sys, os
          print('Python path:', sys.path[:3])
          try:
              from app.core.config import settings
              db_url = getattr(settings, 'DATABASE_URL', None) or getattr(settings, 'database_url', None)
              print('Settings DATABASE_URL:', db_url)
          except Exception as e:
              print('Settings import failed:', e)
          "
          
          # 3. Vérification plugins essentiels (rapide)
          echo "--- Essential checks ---"
          python -c "import pytest_split; print('✓ pytest_split available')"
          pytest --version
          
          # 4. Check tests disponibilité (optimisé)
          echo "--- Test discovery ---"
          TEST_FILES=$(find . -name "*test*.py" -path "*/tests/*" | wc -l)
          echo "Test files found: $TEST_FILES"
          
          # 5. Check Redis connexion
          echo "--- Redis check ---"
          python -c "
          import redis
          try:
              r = redis.Redis.from_url('$REDIS_URL')
              r.ping()
              print('✓ Redis connection OK')
          except Exception as e:
              print('⚠️ Redis connection failed:', e)
          "

      - name: Run unit tests (shard ${{ matrix.shard }})
        env:
          CELERY_TASK_ALWAYS_EAGER: "1"
          STUB_SLACK: "1"
          ALERT_REMINDER_MINUTES: "1"
          SLACK_WEBHOOK: "http://httpbin:80/status/204"
          PYTHONPATH: server
        run: |
          set -euo pipefail
          
          # Configuration de la couverture
          export COVERAGE_FILE=".coverage.unit.${{ matrix.shard }}"
          echo "=== RUNNING UNIT TESTS SHARD ${{ matrix.shard }} ==="
          echo "COVERAGE_FILE=$COVERAGE_FILE"
          echo "Working directory: $(pwd)"
          
          # Tentative avec pytest-split d'abord
          echo ""
          echo "Attempting pytest-split sharding..."
          if pytest -m "unit" --splits 4 --group ${{ matrix.shard }} -n auto \
            --cov=server/app --cov-config=.coveragerc \
            --cov-report=term-missing \
            --cov-fail-under=0 --maxfail=1; then
            
            echo "✓ pytest-split succeeded"
            TESTS_RAN=true
            
          else
            echo ""
            echo "❌ pytest-split failed, trying alternative sharding..."
            
            # Alternative : découpage manuel par nom de fichier
            mapfile -t TEST_FILES < <(find . -name "*test*.py" -path "*/tests/*" -type f | sort)
            TOTAL_FILES=${#TEST_FILES[@]}
            
            if [[ $TOTAL_FILES -eq 0 ]]; then
              echo "❌ No test files found!"
              find . -name "*test*.py" | head -5
              exit 1
            fi
            
            echo "Found $TOTAL_FILES test files, splitting into 4 shards"
            
            # Calcul des indices pour ce shard
            SHARD_SIZE=$(( (TOTAL_FILES + 3) / 4 ))  # arrondi supérieur
            START_IDX=$(( ((${{ matrix.shard }} - 1) * SHARD_SIZE) ))
            END_IDX=$(( START_IDX + SHARD_SIZE ))
            
            echo "Shard ${{ matrix.shard }}: files $START_IDX to $((END_IDX-1))"
            
            # Extraire les fichiers pour ce shard
            SHARD_FILES=()
            for (( i=START_IDX; i<END_IDX && i<TOTAL_FILES; i++ )); do
              SHARD_FILES+=("${TEST_FILES[i]}")
            done
            
            if [[ ${#SHARD_FILES[@]} -eq 0 ]]; then
              echo "⚠️ No test files assigned to shard ${{ matrix.shard }}"
              echo "Creating empty coverage file..."
              echo "# Empty shard - no tests assigned" > "$COVERAGE_FILE"
              TESTS_RAN=false
            else
              echo "Running ${#SHARD_FILES[@]} test files in this shard:"
              printf '  %s\n' "${SHARD_FILES[@]}"
              
              pytest "${SHARD_FILES[@]}" -m "unit" -n auto \
                --cov=server/app --cov-config=.coveragerc \
                --cov-report=term-missing \
                --cov-fail-under=0 --maxfail=1
              
              TESTS_RAN=true
            fi
          fi
          
          echo ""
          echo "=== POST-TEST STATUS ==="
          echo "Tests ran: $TESTS_RAN"
          echo "Current directory files:"
          ls -la .coverage* || echo "No .coverage* files found"

      - name: Normalize coverage filename (shard)
        run: |
          set -euxo pipefail
          TGT=".coverage.unit.${{ matrix.shard }}"
          echo "=== COVERAGE NORMALIZATION ==="
          echo "Target filename: $TGT"
          ls -la
          find . -maxdepth 1 -name ".coverage*" -ls || true
          if [[ -f "$TGT" ]]; then
            echo "✓ Target file already exists: $TGT"
          elif [[ -f ".coverage" ]]; then
            echo "→ Renaming .coverage to $TGT"
            mv .coverage "$TGT"
          else
            echo "# Placeholder - no coverage data" > "$TGT"
          fi
          # Nettoyage et permissions
          rm -f .coverage || true
          chmod 644 "$TGT"
          ls -la "$TGT"
            
      - name: Verify coverage file exists before upload
        run: |
          echo "=== PRE-UPLOAD VERIFICATION ==="
          echo "Checking for .coverage.unit.${{ matrix.shard }}"
          ls -l ".coverage.unit.${{ matrix.shard }}" || echo "❌ Not found in current dir"
          echo "Current dir: $(pwd)"
          echo "Workspace: ${{ github.workspace }}"
          find "${{ github.workspace }}" -name ".coverage.unit.${{ matrix.shard }}" -ls || echo "❌ Not found in workspace" 

      - name: Debug workspace avant upload
        run: |
          echo "=== DEBUG AVANT UPLOAD ==="
          echo "Current directory: $(pwd)"
          echo "GitHub workspace: ${{ github.workspace }}"
          echo "Matrix shard: ${{ matrix.shard }}"
          echo ""
          echo "Listing all coverage files:"
          find . -name ".coverage*" -ls 2>/dev/null || echo "No coverage files found in current dir"
          echo ""
          echo "Listing from GitHub workspace root:"
          find "${{ github.workspace }}" -name ".coverage.unit.${{ matrix.shard }}" -ls 2>/dev/null || echo "Not found in workspace root"
          echo ""
          echo "File existence check:"
          ls -la ".coverage.unit.${{ matrix.shard }}" 2>/dev/null || echo "File does not exist in current dir"

      - name: Prepare for upload
        run: |
          mkdir -p artifacts
          cp .coverage.unit.${{ matrix.shard }} artifacts/coverage_unit_${{ matrix.shard }}
          
      - name: Upload raw coverage (shard ${{ matrix.shard }})
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: coverage-shard-${{ matrix.shard }}
          path: artifacts/coverage_unit_${{ matrix.shard }}
          retention-days: 1
          if-no-files-found: error
        
      - name: Debug après échec upload
        if: failure()
        run: |
          echo "=== DEBUG APRÈS ÉCHEC UPLOAD ==="
          echo "Vérification finale de l'existence du fichier:"
          find . -name "coverage_unit_${{ matrix.shard }}" -exec ls -la {} \; 2>/dev/null || echo "Fichier introuvable"
          echo ""
          echo "Arborescence complète du workspace:"
          pwd
          ls -la
          echo ""
          echo "Processus en cours:"
          ps auxf

  # ---------------------------------------------------------------------------
  # 2) INTEGRATION — OPTIMISÉ (parallèle avec e2e + cache Docker)
  # ---------------------------------------------------------------------------
  integration:
    runs-on: ubuntu-latest
    # OPTIMISATION : Plus de dépendance sur unit - tourne en parallèle
    env:
      STUB_SLACK: "1"
      ALERT_REMINDER_MINUTES: "1"
      KEY: dev-apikey-123
      API: http://localhost:8000
      PGOPTIONS: "-c lock_timeout=5s -c statement_timeout=60000"
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      # OPTIMISATION : Setup Docker Buildx pour cache
      - name: Setup Docker Buildx
        uses: docker/setup-buildx-action@v3

      # OPTIMISATION : Cache Docker layers
      - name: Cache Docker layers
        uses: actions/cache@v4
        with:
          path: /tmp/.buildx-cache
          key: ${{ runner.os }}-buildx-integration-${{ hashFiles('docker/Dockerfile*', 'requirements*.txt') }}
          restore-keys: |
            ${{ runner.os }}-buildx-integration-
            ${{ runner.os }}-buildx-

      - name: Show Docker versions
        run: |
          docker --version
          docker compose version || true

      - name: Ensure curl + jq are present
        run: |
          sudo apt-get update -y
          sudo apt-get install -y curl jq

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      # OPTIMISATION : Cache pip pour integration
      - name: Cache pip (integration)
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-py311-integration-pip-${{ hashFiles('requirements*.txt') }}
          restore-keys: |
            ${{ runner.os }}-py311-integration-pip-
            ${{ runner.os }}-py311-pip-

      - name: Install project deps (integration)
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements-dev.txt
          pip install "psycopg[binary]>=3.1,<4.0"

      - name: Prepare root .env.docker from example
        run: |
          if [ -f .env.example ]; then
            cp .env.example .env.docker
          else
            echo "ERROR: .env.example is missing at repo root" >&2
            exit 1
          fi
          awk 'BEGIN{pslack=0; prem=0}
               /^SLACK_WEBHOOK=/ {print "SLACK_WEBHOOK=http://httpbin:80/status/204"; pslack=1; next}
               /^ALERT_REMINDER_MINUTES=/ {print "ALERT_REMINDER_MINUTES=1"; prem=1; next}
               {print}
               END{
                 if(!pslack) print "SLACK_WEBHOOK=http://httpbin:80/status/204";
                 if(!prem)   print "ALERT_REMINDER_MINUTES=1";
                 print "STUB_SLACK=1";
               }' .env.docker > .env.ci && mv .env.ci .env.docker
          cp .env.docker docker/.env.docker
          sed 's/SLACK_WEBHOOK=.*/SLACK_WEBHOOK=[REDACTED]/' .env.docker

      # OPTIMISATION : Build avec cache
      - name: Start stack (db/redis/api/worker)
        working-directory: docker
        run: |
          # Utilise le cache BuildKit
          BUILDKIT_INLINE_CACHE=1 docker compose --env-file ../.env.docker up -d --build db redis api worker

      # OPTIMISATION : Timeout réduit pour DB (30 au lieu de 60)
      - name: Wait for DB
        working-directory: docker
        run: |
          echo "=== Waiting for database ==="
          for i in {1..30}; do  # Réduit de 60 à 30
            echo "Attempt $i/30..."
            if docker compose --env-file ../.env.docker exec -T db pg_isready -U postgres; then
              echo "✓ Database is ready"
              break
            fi
            if [[ $i -eq 30 ]]; then
              echo "❌ Database failed to start"
              docker compose logs db
              exit 1
            fi
            sleep 1  # Réduit de 2s à 1s
          done

      - name: Run Alembic migrations
        working-directory: docker
        run: |
          echo "=== Running database migrations ==="
          docker compose --env-file ../.env.docker run --rm -w /app/server api alembic -c /app/server/alembic.ini upgrade head

      # OPTIMISATION : Timeout et intervalle réduits pour API
      - name: Wait for API
        run: |
          echo "=== Waiting for API health check ==="
          for i in {1..30}; do  # Réduit de 60 à 30
            echo "Attempt $i/30: checking $API/api/v1/health"
            if curl -fsS -m 3 -H "X-API-Key: $KEY" "$API/api/v1/health"; then  # timeout 3s au lieu de 2s
              echo "✓ API is healthy"
              exit 0
            fi
            if [[ $i -eq 30 ]]; then
              echo "❌ API health check failed"
              echo "=== API logs ==="
              docker compose -f docker/docker-compose.yml logs api | tail -50
              exit 1
            fi
            sleep 1  # Réduit de 2s à 1s
          done

      - name: Sanity env for DB (integration)
        env:
          PYTHONPATH: server
          DATABASE_URL: 'postgresql+psycopg://postgres:postgres@localhost:5432/monitoring?connect_timeout=5'
        run: |
          echo "=== Integration DB sanity check ==="
          echo "DATABASE_URL=$DATABASE_URL"
          python - <<'PY'
          import os
          print("ENV DATABASE_URL =", os.getenv("DATABASE_URL"))
          from app.core.config import settings
          print("settings.DATABASE_URL =", settings.DATABASE_URL)
          PY

      - name: Run integration tests (host)
        env:
          PYTHONPATH: server
          API: ${{ env.API }}
          KEY: ${{ env.KEY }}
          DATABASE_URL: 'postgresql+psycopg://postgres:postgres@localhost:5432/monitoring?connect_timeout=5'
        run: |
          echo "=== Running integration tests ==="
          pytest -m "integration" --no-cov --maxfail=1 -v

      # OPTIMISATION : Logs compressés en cas d'échec
      - name: Dump docker logs on failure
        if: failure()
        working-directory: docker
        run: |
          echo "=== Container status ==="
          docker compose ps || true
          echo ""
          echo "=== API logs (last 200 lines) ==="  # Réduit de 400 à 200
          docker compose logs api | tail -n 200 || true
          echo ""
          echo "=== Worker logs (last 200 lines) ==="  # Réduit de 400 à 200
          docker compose logs worker | tail -n 200 || true
          echo ""
          echo "=== DB logs (last 100 lines) ==="  # Réduit de 200 à 100
          docker compose logs db | tail -n 100 || true

      - name: Stop stack
        if: always()
        working-directory: docker
        run: docker compose --env-file ../.env.docker down -v

  # ---------------------------------------------------------------------------
  # 3) E2E — OPTIMISÉ (parallèle avec integration + cache Docker)
  # ---------------------------------------------------------------------------
  e2e:
    runs-on: ubuntu-latest
    # OPTIMISATION : Plus de dépendance sur integration - tourne en parallèle
    env:
      STUB_SLACK: "1"
      ALERT_REMINDER_MINUTES: "1"
      KEY: dev-apikey-123
      API: http://localhost:8000
      PGOPTIONS: "-c lock_timeout=5s -c statement_timeout=60000"
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      # OPTIMISATION : Setup Docker Buildx pour cache
      - name: Setup Docker Buildx
        uses: docker/setup-buildx-action@v3

      # OPTIMISATION : Cache Docker layers (séparé d'integration)
      - name: Cache Docker layers
        uses: actions/cache@v4
        with:
          path: /tmp/.buildx-cache
          key: ${{ runner.os }}-buildx-e2e-${{ hashFiles('docker/Dockerfile*', 'requirements*.txt') }}
          restore-keys: |
            ${{ runner.os }}-buildx-e2e-
            ${{ runner.os }}-buildx-

      - name: Show Docker versions
        run: |
          docker --version
          docker compose version || true

      - name: Ensure curl + jq are present
        run: |
          sudo apt-get update -y
          sudo apt-get install -y curl jq

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      # OPTIMISATION : Cache pip pour e2e
      - name: Cache pip (e2e)
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-py311-e2e-pip-${{ hashFiles('requirements*.txt') }}
          restore-keys: |
            ${{ runner.os }}-py311-e2e-pip-
            ${{ runner.os }}-py311-pip-

      - name: Install project deps (e2e)
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements-dev.txt
          pip install "psycopg[binary]>=3.1,<4.0"

      - name: Prepare root .env.docker from example
        run: |
          if [ -f .env.example ]; then
            cp .env.example .env.docker
          else
            echo "ERROR: .env.example is missing at repo root" >&2
            exit 1
          fi
          awk 'BEGIN{pslack=0; prem=0}
               /^SLACK_WEBHOOK=/ {print "SLACK_WEBHOOK=http://httpbin:80/status/204"; pslack=1; next}
               /^ALERT_REMINDER_MINUTES=/ {print "ALERT_REMINDER_MINUTES=1"; prem=1; next}
               {print}
               END{
                 if(!pslack) print "SLACK_WEBHOOK=http://httpbin:80/status/204";
                 if(!prem)   print "ALERT_REMINDER_MINUTES=1";
                 print "STUB_SLACK=1";
               }' .env.docker > .env.ci && mv .env.ci .env.docker
          cp .env.docker docker/.env.docker
          sed 's/SLACK_WEBHOOK=.*/SLACK_WEBHOOK=[REDACTED]/' .env.docker

      # OPTIMISATION : Build avec cache (stack minimale pour e2e)
      - name: Start stack (db/redis/api)
        working-directory: docker
        run: |
          # Utilise le cache BuildKit
          BUILDKIT_INLINE_CACHE=1 docker compose --env-file ../.env.docker up -d --build db redis api

      # OPTIMISATION : Timeout réduit pour DB
      - name: Wait for DB
        working-directory: docker
        run: |
          echo "=== Waiting for database ==="
          for i in {1..30}; do  # Réduit de 60 à 30
            echo "Attempt $i/30..."
            if docker compose --env-file ../.env.docker exec -T db pg_isready -U postgres; then
              echo "✓ Database is ready"
              break
            fi
            if [[ $i -eq 30 ]]; then
              echo "❌ Database failed to start"
              docker compose logs db
              exit 1
            fi
            sleep 1  # Réduit de 2s à 1s
          done

      - name: Run Alembic migrations
        working-directory: docker
        run: |
          echo "=== Running database migrations ==="
          docker compose --env-file ../.env.docker run --rm -w /app/server api alembic -c /app/server/alembic.ini upgrade head

      # OPTIMISATION : Timeout et intervalle réduits pour API
      - name: Wait for API to be healthy
        run: |
          echo "=== Waiting for API health check ==="
          for i in {1..30}; do  # Réduit de 60 à 30
            echo "Attempt $i/30: checking $API/api/v1/health"
            if curl -fsS -m 3 -H "X-API-Key: $KEY" "$API/api/v1/health"; then  # timeout 3s
              echo "✓ API is healthy"
              exit 0
            fi
            if [[ $i -eq 30 ]]; then
              echo "❌ API health check failed"
              echo "=== API logs ==="
              docker compose -f docker/docker-compose.yml logs api | tail -50
              exit 1
            fi
            sleep 1  # Réduit de 2s à 1s
          done

      - name: Sanity env for DB (e2e)
        env:
          PYTHONPATH: server
          DATABASE_URL: 'postgresql+psycopg://postgres:postgres@localhost:5432/monitoring?connect_timeout=5'
        run: |
          echo "=== E2E DB sanity check ==="
          echo "DATABASE_URL=$DATABASE_URL"
          python - <<'PY'
          import os
          print("ENV DATABASE_URL =", os.getenv("DATABASE_URL"))
          from app.core.config import settings
          print("settings.DATABASE_URL =", settings.DATABASE_URL)
          PY

      - name: HTTP targets smoke
        env:
          API: ${{ env.API }}
          KEY: ${{ env.KEY }}
        run: |
          echo "=== Running HTTP targets smoke test ==="
          bash scripts/smoke_http_targets.sh

      - name: Run E2E tests
        env:
          PYTHONPATH: server
          API: ${{ env.API }}
          KEY: ${{ env.KEY }}
          STUB_SLACK: "1"
          ALERT_REMINDER_MINUTES: "1"
          DATABASE_URL: 'postgresql+psycopg://postgres:postgres@localhost:5432/monitoring?connect_timeout=5'
        run: |
          echo "=== Running E2E tests ==="
          # E2E : pas de parallélisme xdist (fragile sur stack partagée)
          pytest -m "e2e" --no-cov --maxfail=1 -v

      # OPTIMISATION : Logs compressés en cas d'échec
      - name: Dump docker logs on failure
        if: failure()
        working-directory: docker
        run: |
          echo "=== Container status ==="
          docker compose ps || true
          echo ""
          echo "=== API logs (last 200 lines) ==="  # Réduit de 400 à 200
          docker compose logs api | tail -n 200 || true
          echo ""
          echo "=== DB logs (last 100 lines) ==="  # Réduit de 200 à 100
          docker compose logs db | tail -n 100 || true

      - name: Stop stack
        if: always()
        working-directory: docker
        run: docker compose --env-file ../.env.docker down -v

  # ---------------------------------------------------------------------------
  # 4) AGGREGATE — INCHANGÉ (combine la couverture des 4 shards unit)
  # ---------------------------------------------------------------------------
  aggregate-unit-coverage:
    runs-on: ubuntu-latest
    needs: unit            # Garde la dépendance sur unit seulement
    steps:
      - uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install coverage tool
        run: pip install coverage

      - name: Create coverage directory
        run: mkdir -p coverage-files

      # Télécharger tous les artifacts de couverture
      - name: Download all coverage artifacts
        uses: actions/download-artifact@v4
        with:
          pattern: coverage-shard-*
          path: coverage-files/
          merge-multiple: true

      - name: Debug downloaded files
        run: |
          echo "=== DOWNLOADED ARTIFACTS DEBUG ==="
          echo "Coverage files directory structure:"
          find coverage-files -type f -ls 2>/dev/null || true
          echo ""
          echo "Specifically looking for .coverage* files:"
          find coverage-files -name ".coverage*" -ls 2>/dev/null || true
          echo ""
          echo "All files in coverage-files:"
          find coverage-files -type f -exec ls -lh {} \; 2>/dev/null || true

      - name: Validate and combine coverage
        run: |
          set -euo pipefail
          
          echo "=== COVERAGE COMBINATION PROCESS ==="
          
          # Recherche flexible des fichiers de couverture
          mapfile -t all_files < <(find coverage-files -type f \( -name '.coverage*' -o -name 'coverage-*' \) 2>/dev/null || true)
          
          echo "Found ${#all_files[@]} coverage-related files total"
          
          # Filtrer les fichiers non vides et valides
          valid_files=()
          for file in "${all_files[@]}"; do
            if [[ -s "$file" ]]; then
              # Vérifier que ce n'est pas juste un placeholder
              if grep -q "# Placeholder\|# Empty" "$file" 2>/dev/null; then
                echo "  → Skipping placeholder: $(basename "$file")"
              else
                valid_files+=("$file")
                echo "  → Valid: $(basename "$file") ($(wc -l < "$file" 2>/dev/null || echo "?") lines)"
              fi
            else
              echo "  → Empty: $(basename "$file")"
            fi
          done
          
          echo ""
          echo "Found ${#valid_files[@]} valid coverage files"
          
          if [[ ${#valid_files[@]} -eq 0 ]]; then
            echo "⚠️ WARNING: No valid coverage files found!"
            echo "This could mean:"
            echo "  1. All shards had no tests to run"
            echo "  2. Coverage collection failed everywhere"
            echo "  3. All tests were skipped"
            echo ""
            echo "Creating minimal coverage report..."
            
            # Créer un rapport vide mais valide
            coverage erase || true
            coverage report --show-missing || echo "No data to report"
            coverage xml -o coverage-unit.xml || touch coverage-unit.xml
            coverage html -d htmlcov-unit || mkdir -p htmlcov-unit
            
            echo "✓ Empty coverage reports created"
          else
            echo "Combining ${#valid_files[@]} coverage files..."
            
            # Debug avant combination
            for file in "${valid_files[@]}"; do
              echo "Processing: $file"
              head -5 "$file" 2>/dev/null || echo "Cannot read file header"
            done
            
            coverage combine "${valid_files[@]}"
            
            echo ""
            echo "Generating comprehensive reports..."
            coverage report -m --skip-covered
            coverage xml -o coverage-unit.xml
            coverage html -d htmlcov-unit
            
            echo "✓ Coverage combination completed successfully"
            echo "Reports generated:"
            ls -lh coverage-unit.xml htmlcov-unit/ 2>/dev/null || true
          fi

      - name: Upload combined coverage XML
        uses: actions/upload-artifact@v4
        with:
          name: coverage-unit-xml
          path: coverage-unit.xml

      - name: Upload HTML report (unit)
        uses: actions/upload-artifact@v4
        with:
          name: htmlcov-unit
          path: htmlcov-unit/
          if-no-files-found: ignore


  # ---------------------------------------------------------------------------
  # 5) FINAL GATE — INCHANGÉ (assure que tout est vert)
  # ---------------------------------------------------------------------------
  gate:
    name: all-checks
    runs-on: ubuntu-latest
    needs: [unit, aggregate-unit-coverage, integration, e2e]  # <-- adapte si tes noms diffèrent
    steps:
      - run: echo "all green ✅"  