#  ─────────────────────────────────────────────────────────────────────────────
# CI en 4 étapes principales : unit → (integration ∥ e2e) → aggregate-unit-coverage → gate
#
# OPTIMISATIONS :
# - Jobs integration/e2e en parallèle (accélère fortement la CI)
# - Cache pip + BuildKit pour Docker
# - Redis en “services” pour les unit (sans Compose)
# - Sharding des unit (pytest-split), avec fallback si nécessaire
# - Agrégation coverage unit FIABLE : téléchargement *explicite* des 4 artifacts
#
# COMPORTEMENT :
# - unit : DB SQLite in-memory, REDIS service GH Actions, couverture **par shard**
# - integration / e2e : stack Docker, migrations, tests host (pas de coverage gate ici)
# - aggregate-unit-coverage : combine les 4 .coverage.unit.N et produit coverage-unit.xml
# - gate : unique statut final à marquer comme “required” dans les protections de branche
# ──────────────────────────────────────────────────────────────────────────────

name: CI

on:
  push:
    branches: [ main ]
  pull_request:

concurrency:
  group: ci-${{ github.ref }}
  cancel-in-progress: true

permissions:
  contents: read

jobs:

  # ────────────────────────────────────────────────────────────────────────────
  # 1) UNIT — sharding x4 + xdist + coverage par shard
  # ────────────────────────────────────────────────────────────────────────────
  unit:
    runs-on: ubuntu-latest

    # Redis natif via “services” (évite Compose pour les unit)
    services:
      redis:
        image: redis:7-alpine
        ports: [ "6379:6379" ]
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

    strategy:
      fail-fast: false
      matrix:
        shard: [1, 2, 3, 4]

    env:
      PYTHONPATH: server
      ENV_FILE: /dev/null
      DATABASE_URL: 'sqlite+pysqlite:///:memory:'
      REDIS_URL: redis://localhost:6379/0
      STUB_SLACK: "1"
      ALERT_REMINDER_MINUTES: "1"
      KEY: dev-apikey-123
      API: http://localhost:8000

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Ensure curl + jq are present
        run: |
          sudo apt-get update -y
          sudo apt-get install -y curl jq

      - name: Setup Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      # cache pip précis (dépendances unit)
      - name: Cache pip (unit)
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-py311-unit-pip-${{ hashFiles('requirements*.txt') }}
          restore-keys: |
            ${{ runner.os }}-py311-unit-pip-
            ${{ runner.os }}-py311-pip-

      - name: Install deps (unit)
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements-dev.txt
          pip install pytest-split

      - name: Quick env diagnostics
        run: |
          echo "PYTHONPATH=$PYTHONPATH"
          echo "DATABASE_URL=$DATABASE_URL"
          echo "REDIS_URL=$REDIS_URL"
          python - <<'PY'
          import sys, os
          print("sys.path[:3] =", sys.path[:3])
          try:
              from app.core.config import settings
              print("settings.DATABASE_URL =", getattr(settings,"DATABASE_URL",None) or getattr(settings,"database_url",None))
          except Exception as e:
              print("Import settings FAILED:", e)
          PY

      - name: Run unit tests (shard ${{ matrix.shard }})
        env:
          CELERY_TASK_ALWAYS_EAGER: "1"
          STUB_SLACK: "1"
          ALERT_REMINDER_MINUTES: "1"
          SLACK_WEBHOOK: "http://httpbin:80/status/204"
        run: |
          set -euo pipefail
          export COVERAGE_FILE=".coverage.unit.${{ matrix.shard }}"
          echo "COVERAGE_FILE=${COVERAGE_FILE}"

          echo "→ Trying pytest-split…"
          if pytest -m "unit" --splits 4 --group ${{ matrix.shard }} -n auto \
                --cov=server/app --cov-config=.coveragerc \
                --cov-report=term-missing --cov-fail-under=0 --maxfail=1; then
            echo "✓ pytest-split OK"
          else
            echo "❌ pytest-split failed, fallback by file slicing"
            mapfile -t TEST_FILES < <(find server/tests -type f -name "*test*.py" | sort)
            TOTAL=${#TEST_FILES[@]}
            if [[ $TOTAL -eq 0 ]]; then echo "No test files"; exit 1; fi
            SIZE=$(( (TOTAL + 3) / 4 ))
            START=$(( (${{ matrix.shard }} - 1) * SIZE ))
            END=$(( START + SIZE ))
            SLICE=( "${TEST_FILES[@]:$START:$SIZE}" )
            if [[ ${#SLICE[@]} -eq 0 ]]; then
              echo "# Placeholder - no tests assigned" > "$COVERAGE_FILE"
            else
              printf '  %s\n' "${SLICE[@]}"
              pytest -n auto "${SLICE[@]}" -m "unit" \
                --cov=server/app --cov-config=.coveragerc \
                --cov-report=term-missing --cov-fail-under=0 --maxfail=1
            fi
          fi

          # Normalisation de nom (au cas où coverage ait écrit ./.coverage)
          if [[ -f .coverage && ! -f "$COVERAGE_FILE" ]]; then mv .coverage "$COVERAGE_FILE"; fi
          ls -la "$COVERAGE_FILE" || { echo "Missing $COVERAGE_FILE"; exit 1; }

      - name: Upload shard coverage artifact
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: coverage-shard-${{ matrix.shard }}
          path: .coverage.unit.${{ matrix.shard }}
          retention-days: 2
          if-no-files-found: error

  # ────────────────────────────────────────────────────────────────────────────
  # 2) INTEGRATION — en parallèle (Docker + migrations + tests host)
  # ────────────────────────────────────────────────────────────────────────────
  integration:
    runs-on: ubuntu-latest
    env:
      STUB_SLACK: "1"
      ALERT_REMINDER_MINUTES: "1"
      KEY: dev-apikey-123
      API: http://localhost:8000
      PGOPTIONS: "-c lock_timeout=5s -c statement_timeout=60000"

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Docker Buildx (cache)
        uses: docker/setup-buildx-action@v3

      - name: Cache Docker layers
        uses: actions/cache@v4
        with:
          path: /tmp/.buildx-cache
          key: ${{ runner.os }}-buildx-integration-${{ hashFiles('server/Dockerfile', 'requirements*.txt', 'docker/**') }}
          restore-keys: |
            ${{ runner.os }}-buildx-integration-
            ${{ runner.os }}-buildx-

      - name: Ensure curl + jq are present
        run: |
          sudo apt-get update -y
          sudo apt-get install -y curl jq

      - name: Setup Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Cache pip (integration)
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-py311-integ-pip-${{ hashFiles('requirements*.txt') }}
          restore-keys: |
            ${{ runner.os }}-py311-integ-pip-
            ${{ runner.os }}-py311-pip-

      - name: Install deps (integration)
        run: |
          python -m pip install -U pip
          pip install -r requirements-dev.txt
          pip install "psycopg[binary]>=3.1,<4.0"

      - name: Prepare .env.docker
        run: |
          if [ -f .env.example ]; then cp .env.example .env.docker; else echo "Missing .env.example" >&2; exit 1; fi
          awk 'BEGIN{ps=0; pr=0}
               /^SLACK_WEBHOOK=/ {print "SLACK_WEBHOOK=http://httpbin:80/status/204"; ps=1; next}
               /^ALERT_REMINDER_MINUTES=/ {print "ALERT_REMINDER_MINUTES=1"; pr=1; next}
               {print}
               END{ if(!ps) print "SLACK_WEBHOOK=http://httpbin:80/status/204";
                    if(!pr) print "ALERT_REMINDER_MINUTES=1";
                    print "STUB_SLACK=1"; }' .env.docker > .env.tmp && mv .env.tmp .env.docker
          cp .env.docker docker/.env.docker
          sed -n 's/SLACK_WEBHOOK=.*/SLACK_WEBHOOK=[REDACTED]/p' .env.docker || true

      - name: Start stack (db/redis/api/worker)
        working-directory: docker
        run: |
          BUILDKIT_INLINE_CACHE=1 docker compose --env-file ../.env.docker up -d --build db redis api worker

      - name: Wait for DB (≤30s)
        working-directory: docker
        run: |
          for i in {1..30}; do
            if docker compose --env-file ../.env.docker exec -T db pg_isready -U postgres; then exit 0; fi
            sleep 1
          done
          echo "DB not ready"; docker compose logs db; exit 1

      - name: Alembic migrations
        working-directory: docker
        run: docker compose --env-file ../.env.docker run --rm -w /app/server api alembic -c /app/server/alembic.ini upgrade head

      - name: Wait API (≤30s)
        run: |
          for i in {1..30}; do
            if curl -fsS -m 3 -H "X-API-Key: $KEY" "$API/api/v1/health"; then exit 0; fi
            sleep 1
          done
          echo "API not healthy"; docker compose -f docker/docker-compose.yml logs api | tail -50; exit 1

      - name: Run integration tests (host)
        env:
          PYTHONPATH: server
          DATABASE_URL: 'postgresql+psycopg://postgres:postgres@localhost:5432/monitoring?connect_timeout=5'
        run: |
          echo "DATABASE_URL=$DATABASE_URL"
          pytest -m "integration" --no-cov --maxfail=1 -v

      - name: Dump docker logs on failure
        if: failure()
        working-directory: docker
        run: |
          docker compose ps || true
          docker compose logs api | tail -n 200 || true
          docker compose logs worker | tail -n 200 || true
          docker compose logs db | tail -n 100 || true

      - name: Stop stack
        if: always()
        working-directory: docker
        run: docker compose --env-file ../.env.docker down -v

  # ────────────────────────────────────────────────────────────────────────────
  # 3) E2E — en parallèle (Docker + migrations + smoke + tests e2e)
  # ────────────────────────────────────────────────────────────────────────────
  e2e:
    runs-on: ubuntu-latest
    env:
      STUB_SLACK: "1"
      ALERT_REMINDER_MINUTES: "1"
      KEY: dev-apikey-123
      API: http://localhost:8000
      PGOPTIONS: "-c lock_timeout=5s -c statement_timeout=60000"

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Docker Buildx (cache)
        uses: docker/setup-buildx-action@v3

      - name: Cache Docker layers
        uses: actions/cache@v4
        with:
          path: /tmp/.buildx-cache
          key: ${{ runner.os }}-buildx-e2e-${{ hashFiles('server/Dockerfile', 'requirements*.txt', 'docker/**') }}
          restore-keys: |
            ${{ runner.os }}-buildx-e2e-
            ${{ runner.os }}-buildx-

      - name: Ensure curl + jq are present
        run: |
          sudo apt-get update -y
          sudo apt-get install -y curl jq

      - name: Setup Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Cache pip (e2e)
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-py311-e2e-pip-${{ hashFiles('requirements*.txt') }}
          restore-keys: |
            ${{ runner.os }}-py311-e2e-pip-
            ${{ runner.os }}-py311-pip-

      - name: Install deps (e2e)
        run: |
          python -m pip install -U pip
          pip install -r requirements-dev.txt
          pip install "psycopg[binary]>=3.1,<4.0"

      - name: Prepare .env.docker
        run: |
          if [ -f .env.example ]; then cp .env.example .env.docker; else echo "Missing .env.example" >&2; exit 1; fi
          awk 'BEGIN{ps=0; pr=0}
               /^SLACK_WEBHOOK=/ {print "SLACK_WEBHOOK=http://httpbin:80/status/204"; ps=1; next}
               /^ALERT_REMINDER_MINUTES=/ {print "ALERT_REMINDER_MINUTES=1"; pr=1; next}
               {print}
               END{ if(!ps) print "SLACK_WEBHOOK=http://httpbin:80/status/204";
                    if(!pr) print "ALERT_REMINDER_MINUTES=1";
                    print "STUB_SLACK=1"; }' .env.docker > .env.tmp && mv .env.tmp .env.docker
          cp .env.docker docker/.env.docker
          sed -n 's/SLACK_WEBHOOK=.*/SLACK_WEBHOOK=[REDACTED]/p' .env.docker || true

      - name: Start stack (db/redis/api)
        working-directory: docker
        run: |
          BUILDKIT_INLINE_CACHE=1 docker compose --env-file ../.env.docker up -d --build db redis api

      - name: Wait for DB (≤30s)
        working-directory: docker
        run: |
          for i in {1..30}; do
            if docker compose --env-file ../.env.docker exec -T db pg_isready -U postgres; then exit 0; fi
            sleep 1
          done
          echo "DB not ready"; docker compose logs db; exit 1

      - name: Alembic migrations
        working-directory: docker
        run: docker compose --env-file ../.env.docker run --rm -w /app/server api alembic -c /app/server/alembic.ini upgrade head

      - name: Wait for API (≤30s)
        run: |
          for i in {1..30}; do
            if curl -fsS -m 3 -H "X-API-Key: $KEY" "$API/api/v1/health"; then exit 0; fi
            sleep 1
          done
          echo "API not healthy"; docker compose -f docker/docker-compose.yml logs api | tail -50; exit 1

      - name: HTTP targets smoke
        env:
          API: ${{ env.API }}
          KEY: ${{ env.KEY }}
        run: bash scripts/smoke_http_targets.sh

      - name: Run E2E tests
        env:
          PYTHONPATH: server
          DATABASE_URL: 'postgresql+psycopg://postgres:postgres@localhost:5432/monitoring?connect_timeout=5'
        run: |
          pytest -m "e2e" --no-cov --maxfail=1 -v

      - name: Dump docker logs on failure
        if: failure()
        working-directory: docker
        run: |
          docker compose ps || true
          docker compose logs api | tail -n 200 || true
          docker compose logs db | tail -n 100 || true

      - name: Stop stack
        if: always()
        working-directory: docker
        run: docker compose --env-file ../.env.docker down -v

  # ────────────────────────────────────────────────────────────────────────────
  # 4) AGGREGATE — combine FIABLE (télécharge explicitement shard 1..4)
  # ────────────────────────────────────────────────────────────────────────────
  aggregate-unit-coverage:
    runs-on: ubuntu-latest
    needs: unit

    steps:
      - uses: actions/checkout@v4

      - name: Setup Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install coverage.py
        run: pip install coverage

      - name: Create coverage staging dir
        run: mkdir -p coverage-files

      # Téléchargements EXPLICITES (évite les soucis de pattern/merge)
      - name: Download shard 1
        uses: actions/download-artifact@v4
        with:
          name: coverage-shard-1
          path: coverage-files/
      - name: Download shard 2
        uses: actions/download-artifact@v4
        with:
          name: coverage-shard-2
          path: coverage-files/
      - name: Download shard 3
        uses: actions/download-artifact@v4
        with:
          name: coverage-shard-3
          path: coverage-files/
      - name: Download shard 4
        uses: actions/download-artifact@v4
        with:
          name: coverage-shard-4
          path: coverage-files/

      - name: Debug downloaded files
        run: |
          echo "=== coverage-files content ==="
          find coverage-files -maxdepth 2 -type f -ls || true
          echo ""
          echo "Looking for unit data files:"
          find coverage-files -type f -name ".coverage.unit.*" -ls || true

      - name: Combine + report (unit)
        run: |
          set -euo pipefail
          mapfile -t files < <(find coverage-files -type f -name ".coverage.unit.*" -size +0c | sort)
          echo "Found ${#files[@]} files"
          if [[ ${#files[@]} -eq 0 ]]; then
            echo "ERROR: No unit coverage files downloaded. Check shard jobs."
            exit 1
          fi
          coverage combine "${files[@]}"
          coverage report -m --skip-covered
          coverage xml -o coverage-unit.xml
          coverage html -d htmlcov-unit
          ls -lh coverage-unit.xml || true

      - name: Upload combined XML (unit)
        uses: actions/upload-artifact@v4
        with:
          name: coverage-unit-xml
          path: coverage-unit.xml

      - name: Upload HTML report (unit)
        uses: actions/upload-artifact@v4
        with:
          name: htmlcov-unit
          path: htmlcov-unit/
          if-no-files-found: ignore

  # ────────────────────────────────────────────────────────────────────────────
  # 5) GATE — statut final unique à rendre “required” dans Branch protection
  # ────────────────────────────────────────────────────────────────────────────
  gate:
    name: all-checks
    runs-on: ubuntu-latest
    needs: [unit, aggregate-unit-coverage, integration, e2e]
    steps:
      - run: echo "all green ✅"
